{"fairseq_preprocess_args": {"--source-lang": ["en"], "--target-lang": ["actions"], "--trainpref": ["DATA/BIOAMR-preprocess//oracles/amr3_o5+Word100//train"], "--validpref": ["DATA/BIOAMR-preprocess//oracles/amr3_o5+Word100//dev"], "--testpref": ["DATA/BIOAMR-preprocess//oracles/amr3_o5+Word100//test"], "--destdir": ["DATA/BIOAMR-preprocess//features/amr3_o5+Word100_RoBERTa-large-top24/"], "--workers": ["1"], "--pretrained-embed": ["roberta.large"], "--bert-layers": ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24"], "--machine-type": ["AMR"], "--machine-rules": ["DATA/BIOAMR-preprocess//oracles/amr3_o5+Word100//train.rules.json"]}, "fairseq_train_args": {"data": [], "--max-epoch": ["100"], "--arch": ["stack_transformer_6x6_nopos"], "--optimizer": ["adam"], "--adam-betas": ["'(0.9,0.98)'"], "--clip-norm": ["0.0"], "--lr-scheduler": ["inverse_sqrt"], "--warmup-init-lr": ["1e-07"], "--warmup-updates": ["4000"], "--pretrained-embed-dim": ["1024"], "--lr": ["0.0005"], "--min-lr": ["1e-09"], "--dropout": ["0.3"], "--weight-decay": ["0.0"], "--criterion": ["label_smoothed_cross_entropy"], "--label-smoothing": ["0.01"], "--keep-last-epochs": ["40"], "--max-tokens": ["3584"], "--log-format": ["json"], "--device-id": ["2"], "--distributed-world-size": ["1"]}}